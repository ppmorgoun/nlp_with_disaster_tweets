{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3af06426",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df_train = pd.read_csv('./nlp-getting-started/train.csv', index_col='id')\n",
    "df_test = pd.read_csv('./nlp-getting-started/test.csv', index_col='id')\n",
    "\n",
    "# real disaster = 1, not a disaster = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "71914bda",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   keyword location                                               text  target\n",
       "id                                                                            \n",
       "1      NaN      NaN  Our Deeds are the Reason of this #earthquake M...       1\n",
       "4      NaN      NaN             Forest fire near La Ronge Sask. Canada       1\n",
       "5      NaN      NaN  All residents asked to 'shelter in place' are ...       1\n",
       "6      NaN      NaN  13,000 people receive #wildfires evacuation or...       1\n",
       "7      NaN      NaN  Just got sent this photo from Ruby #Alaska as ...       1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "416fbe2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7613, 4)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c6c28e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4296597924602653\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5703402075397347"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculating a baseline\n",
    "\n",
    "avg_target = sum(df_train['target']) / len(df_train['target'])\n",
    "print(avg_target)\n",
    "df_train['baseline_pred'] = 0 if avg_target < 0.5 else 1\n",
    "train_baseline_accuracy = len(df_train.loc[df_train['target']==df_train['baseline_pred']]) / len(df_train)\n",
    "train_baseline_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7150c337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline is 57% accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ad4373",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "39323fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Tweet Stats --\n",
      "Number of tweets in test dataset = 7613\n",
      "Average words per tweet = 14.9\n",
      "Ranging between 1 words and 31 words\n"
     ]
    }
   ],
   "source": [
    "df_train['split_text'] = df_train['text'].apply(lambda x: x.split())\n",
    "df_train['split_text']\n",
    "\n",
    "avg_words_per_tweet = np.mean(df_train['split_text'].apply(lambda x: len(x)))\n",
    "min_tweet_len = np.min(df_train['split_text'].apply(lambda x: len(x)))\n",
    "max_tweet_len = np.max(df_train['split_text'].apply(lambda x: len(x)))\n",
    "print(f\"-- Tweet Stats --\\nNumber of tweets in test dataset = {len(df_train)}\\nAverage words per tweet = {avg_words_per_tweet.round(2)}\\nRanging between {min_tweet_len} words and {max_tweet_len} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bda7be",
   "metadata": {},
   "source": [
    "## Rough plan of attack:\n",
    "Clean text\n",
    "\n",
    "TFIDF (generate features)\n",
    "\n",
    "ML Models:\n",
    "\n",
    "1). Random forest\n",
    "\n",
    "2). Deep NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f431fe66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00' '000' '0000' ... 'ûónegligence' 'ûótech' 'ûówe']\n",
      "(7613, 21637)\n"
     ]
    }
   ],
   "source": [
    "# Generate features for each tweet\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer as tfidf\n",
    "vectorizer = tfidf()\n",
    "X = vectorizer.fit_transform(df_train['text'])\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862f272b",
   "metadata": {},
   "source": [
    "### Without cleaning the tweets first we are left with a giant sparse matrix with a shape of (7613, 21637)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ccdea771",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/petr/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/petr/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download nltk stopwords seperately due to SSL errors\n",
    "import nltk\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a76b2c21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "def preprocess_tweets(df, stem=False, lemmatize=False):\n",
    "    \"\"\"\n",
    "    Function that takes a kaggle provided tweet dataset, identifies the text column, and cleans the words in the tweets\n",
    "    \n",
    "    Cleaning steps:\n",
    "    1). Removes all non alphanumeric characters excluding the hashtag #\n",
    "    2). Removes common english stopwords\n",
    "    3). Sets all characters to lowercase\n",
    "    4). OPTIONAL: destems each word in each tweet\n",
    "    5). OPTIONAL: delemmatizes each word in each tweet\n",
    "    \n",
    "    \n",
    "    RETURNS:\n",
    "    A list of preprocessed tweets\n",
    "    \"\"\"\n",
    "    textRaw = df['text']\n",
    "    # Remove all non-alphanumeric characters excluding the hashtag. Remove any website links. Set all words to lowercase\n",
    "    text01 = [re.sub(\"[^#a-zA-Z 0-9]+|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", x).lower() for x in textRaw]\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop = stopwords.words('english')\n",
    "    text02 = [\" \".join([word for word in tweet.split() if word not in (stop)]) for tweet in text01]\n",
    "    \n",
    "    # Stemming\n",
    "    if stem==True:\n",
    "        stemmer = PorterStemmer()\n",
    "        text03 = [\" \".join([stemmer.stem(word) for word in tweet.split()]) for tweet in text02]\n",
    "\n",
    "    # Lemmatization\n",
    "    if lemmatize==False:\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        if stem == True:\n",
    "            text04 = [\" \".join([lemmatizer.lemmatize(word) for word in tweet.split()]) for tweet in text03]\n",
    "        else:\n",
    "            text05 = [\" \".join([lemmatizer.lemmatize(word) for word in tweet.split()]) for tweet in text02]\n",
    "\n",
    "    if stem == lemmatize == False:\n",
    "        return text02\n",
    "    elif stem == True and lemmatize == False:\n",
    "        return text03\n",
    "    elif stem == False and lemmatize == True:\n",
    "        return text05\n",
    "    else:\n",
    "        return text04"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa8bdfb",
   "metadata": {},
   "source": [
    "### It seems like the stemming and lemmatizer are cutting off more character than they should be, so for now I will proceed with the modelling using text02 (all preprocessing less stemming and lemmatization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "948cbec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0011' '001116' '0025' ... 'zurich' 'zxathetis' 'zzzz']\n",
      "(7613, 17808)\n"
     ]
    }
   ],
   "source": [
    "# Generating tfidf feature matrix using preprocessed words\n",
    "\n",
    "vectorizer = tfidf()\n",
    "\n",
    "text02 = preprocess_tweets(df_train)\n",
    "\n",
    "X2 = vectorizer.fit_transform(text02)\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(X2.shape)\n",
    "\n",
    "# Testing to see how big the vocab would be with stemming and lemmatization\n",
    "\n",
    "# X3 = vectorizer.fit_transform(text03)\n",
    "# print(vectorizer.get_feature_names_out())\n",
    "# print(f\"Shape of tfidf matrix with de-stemmed tweets = {X3.shape}\")\n",
    "\n",
    "# X4 = vectorizer.fit_transform(text04)\n",
    "# print(vectorizer.get_feature_names_out())\n",
    "# print(f\"Shape of tfidf matrix with de-lemmatized tweets = {X4.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a5021c",
   "metadata": {},
   "source": [
    "### Results: preprocessing the tweets cut down size of vocabulary from ~21.6K to 17.8K\n",
    "### De-stemmed vocab size = 14.8K, de-lemmatized vocab size = 16.6K"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee90b972",
   "metadata": {},
   "source": [
    "## Modelling time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3f676a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating validation set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = df_train['target']\n",
    "X_train, X_val, y_train, y_val = train_test_split(X2, y, test_size=0.2)\n",
    "X_test = df_test['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "85775a62",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- EXPERIMENT RESULTS --\n",
      "TRAIN BASELINE ACCURACY = 0.5703402075397347\n",
      "RF TRAINING SET ACCURACY = 0.9883415435139573\n",
      "VALIDATION SET ACCURACY = 0.7708470124753776\n"
     ]
    }
   ],
   "source": [
    "# Random Forest classifier\n",
    "from sklearn.ensemble import RandomForestClassifier as rf\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "clf = rf()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = clf.predict(X_train)\n",
    "y_train_acc = accuracy_score(y_train, y_train_pred)\n",
    "\n",
    "y_val_pred = clf.predict(X_val)\n",
    "y_val_acc = accuracy_score(y_val, y_val_pred)\n",
    "\n",
    "print(F\"-- EXPERIMENT RESULTS --\\nTRAIN BASELINE ACCURACY = {train_baseline_accuracy}\\nRF TRAINING SET ACCURACY = {y_train_acc}\"\n",
    "    F\"\\nVALIDATION SET ACCURACY = {y_val_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "66f603cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<6090x17808 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 54849 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "96970f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/petr/Documents/fun_stuff/nlp_with_disaster_tweets/nlp1/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:39:26] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "-- EXPERIMENT RESULTS --\n",
      "TRAIN BASELINE ACCURACY = 0.5703402075397347\n",
      "XGB TRAINING SET ACCURACY = 0.9022988505747126\n",
      "XGB VALIDATION SET ACCURACY = 0.7636244254760342\n"
     ]
    }
   ],
   "source": [
    "# XGBoost\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model = XGBClassifier(n_estimators=300, max_depth=5)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_train_acc = accuracy_score(y_train, y_train_pred)\n",
    "\n",
    "y_val_pred = model.predict(X_val)\n",
    "y_val_acc = accuracy_score(y_val, y_val_pred)\n",
    "\n",
    "print(F\"-- EXPERIMENT RESULTS --\\nTRAIN BASELINE ACCURACY = {train_baseline_accuracy}\\nXGB TRAINING SET ACCURACY = {y_train_acc}\"\n",
    "    F\"\\nXGB VALIDATION SET ACCURACY = {y_val_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "537e9c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submitting my first Kaggle submission\n",
    "\n",
    "X_test_cleaned = preprocess_tweets(df_test)\n",
    "\n",
    "X_test = vectorizer.transform(X_test_cleaned)\n",
    "\n",
    "sample_sub = pd.read_csv(\"./nlp-getting-started/sample_submission.csv\", index_col = 'id')\n",
    "y_test_pred = clf.predict(X_test)\n",
    "sample_sub['target'] = y_test_pred\n",
    "sample_sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "49ce1b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving my first kaggle submission\n",
    "import os.path\n",
    "from os import path\n",
    "\n",
    "def save_submission(pred):\n",
    "    counter = 0\n",
    "    name = F\"kaggle_submission_00{counter}\"\n",
    "    while path.exists(f\"{name}.csv\"):\n",
    "        counter += 1\n",
    "        if counter == 100:\n",
    "            break\n",
    "    pred.to_csv(f\"{name}{counter}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "72c4772a",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_submission(sample_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb13ec39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
